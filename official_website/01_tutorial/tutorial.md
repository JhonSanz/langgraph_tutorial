# Encadenamiento con edges, estado y nodos.

`self.tools_by_name = {tool.name: tool for tool in tools}`


Cada tool que pasas en la lista tools es un objeto de tipo Tool de LangChain.

Cuando creas un tool (ej: travily_tool), normalmente lo defines con algo como:

```python
from langchain_core.tools import tool

@tool
def travily_tool(query: str):
    """Busca informaci√≥n en Travily."""
    return {"result": f"Buscando {query}..."}
```


Ese decorador (@tool) convierte tu funci√≥n en un objeto StructuredTool, y ese objeto siempre tiene atributos como:

- .name ‚Üí el nombre de la herramienta (por defecto el nombre de la funci√≥n, "travily_tool")
- .description ‚Üí para que el LLM sepa qu√© hace
- .args_schema ‚Üí qu√© argumentos acepta

üëâ Entonces, cuando haces:

`{tool.name: tool for tool in tools}`


Est√°s construyendo un diccionario tipo:

```python
{
  "travily_tool": <StructuredTool travily_tool>,
  ...
}
```

Eso sirve para luego llamar a la herramienta correcta cuando el LLM diga "name": "travily_tool".

--- 

`inputs.get("messages", [])`

Esto pasa porque cada nodo del grafo recibe un diccionario con el state actual.
En tu caso, State es b√°sicamente:

```python
from typing import TypedDict

class State(TypedDict):
    messages: list


State = TypedDict("State", {"messages": list})
```


y tu grafo siempre pasa el estado as√≠:

`{"messages": [...]}`


#### Ejemplo en ejecuci√≥n:

El usuario manda "hola" ‚Üí tu grafo inicializa el state como:

`{"messages": [HumanMessage(content="hola")]}`


Ese diccionario va fluyendo entre nodos (chatbot, tools, etc).

En BasicToolNode.__call__, el nodo recibe ese diccionario y hace:

`inputs.get("messages", [])`


para acceder a la lista de mensajes y procesar el √∫ltimo.

üëâ Es la manera est√°ndar de leer el historial de mensajes que va cargando LangGraph.

--- 

`for tool_call in message.tool_calls`

Esto viene del AIMessage generado por el LLM cuando se le hace un .bind_tools(...).

#### Ejemplo:

Sin tools, el LLM devuelve un AIMessage con solo content.

Con tools, el LLM puede devolver un AIMessage con una lista en .tool_calls.

Esa propiedad .tool_calls la rellena LangChain autom√°ticamente cuando el modelo responde con el formato de ‚Äúinvocar herramientas‚Äù (siguiendo el JSON schema que se le da en el binding).

Ejemplo de AIMessage con tool_calls:

```python
AIMessage(
  content="",
  tool_calls=[
    {
      "name": "travily_tool",
      "args": {"query": "√∫ltimas noticias de Ucrania"},
      "id": "call_123"
    }
  ]
)
```


Entonces:

route_tools revisa si el √∫ltimo mensaje (ai_message) tiene .tool_calls.

Si s√≠ ‚Üí lo manda al nodo "tools".

Si no ‚Üí lo manda a END.

üëâ En BasicToolNode, haces un loop:

```python
for tool_call in message.tool_calls:
    tool_result = self.tools_by_name[tool_call["name"]].invoke(tool_call["args"])
```

Y eso ejecuta la funci√≥n real de Python que corresponde a la herramienta.

‚úÖ En resumen:

- tool.name ‚Üí viene de c√≥mo LangChain define las herramientas (StructuredTool).
- inputs["messages"] ‚Üí es el estado que fluye en el grafo.
- .tool_calls ‚Üí es un campo que LangChain mete en el AIMessage cuando el modelo pide usar una herramienta.




---


Entonces el flujo es:

1. El chatbot genera una respuesta.
2. route_tools mira el √∫ltimo mensaje.
   - Si hay tool_calls ‚Üí devuelve "tools" (ese es el nodo BasicToolNode).
   - Si no hay ‚Üí devuelve END.
3. El grafo consulta el mapa de resoluciones y dice:
   - "tools" ‚Üí ir al nodo "tools".
   - END ‚Üí terminar.
4. Una vez que se ejecuta el nodo "tools", agregaste este edge:
`graph_builder.add_edge("tools", "chatbot")`
o sea que al acabar un tool, el flujo vuelve al chatbot.